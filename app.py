# -*- coding: utf-8 -*-
# =============================================================
# app_part1.py â€” ê³µí†µ ìœ í‹¸/ë³€í™˜/í°íŠ¸/PDF/HWP íŒŒì´í”„ë¼ì¸ (hwp5txt ìš°ì„  â†’ unoconv í´ë°±)
# =============================================================

import os
import re
import io
import sys
import zipfile
import shutil
import tempfile
import subprocess
from io import BytesIO
from datetime import datetime
from urllib.parse import urlparse, unquote

import streamlit as st
import pandas as pd
import numpy as np
import plotly.express as px

# =========================
# ì „ì—­ ì„¤ì • & ê³µìš© ìœ í‹¸
# =========================

st.set_page_config(
    page_title="ì¡°ë‹¬ì…ì°° ë¶„ì„ ì‹œìŠ¤í…œ",
    layout="wide",
    initial_sidebar_state="expanded",
)

st.markdown(
    """
    <meta name="robots" content="noindex,nofollow">
    <meta name="googlebot" content="noindex,nofollow">
    """,
    unsafe_allow_html=True,
)

def _redact_secrets(text: str) -> str:
    if not isinstance(text, str):
        return text
    text = re.sub(r'sk-[A-Za-z0-9_\-]{20,}', '[REDACTED_KEY]', text)
    text = re.sub(r'OPENAI_API_KEY\s*=\s*["\'].*?["\']', 'OPENAI_API_KEY="[REDACTED]"', text)
    return text

def _decode_best_effort(raw: bytes) -> str:
    for enc in ("utf-8-sig", "utf-8", "cp949", "euc-kr"):
        try:
            return raw.decode(enc)
        except Exception:
            continue
    return raw.decode("utf-8", errors="ignore")

def _is_url(val: str) -> bool:
    s = str(val).strip()
    return s.startswith("http://") or s.startswith("https://")

def _filename_from_url(url: str) -> str:
    try:
        path = urlparse(url).path
        if not path:
            return url
        return unquote(path.split("/")[-1]) or url
    except Exception:
        return url

def _which(cmd: str):
    try:
        return shutil.which(cmd)
    except Exception:
        return None

# =========================
# PDF ìƒì„± (Markdown/í…ìŠ¤íŠ¸ â†’ PDF)
# =========================

@st.cache_resource
def _scan_local_fonts():
    fonts = []
    base = os.path.dirname(__file__) if "__file__" in globals() else "."
    fonts_dir = os.path.join(base, "fonts")
    if os.path.isdir(fonts_dir):
        for fname in os.listdir(fonts_dir):
            lower = fname.lower()
            if lower.endswith((".ttf", ".otf", ".ttc")):
                path = os.path.join(fonts_dir, fname)
                ext = "ttc" if lower.endswith(".ttc") else ("ttf" if lower.endswith(".ttf") else "otf")
                name = os.path.splitext(fname)[0]
                fonts.append((name, path, ext))
    fonts.sort(key=lambda x: {"ttf":0, "otf":1, "ttc":2}.get(x[2], 3))
    return fonts

@st.cache_resource
def _resolve_korean_font_candidates():
    cands = []
    cands.extend(_scan_local_fonts())
    sys_ttf = [
        ("/Library/Fonts/NanumGothic.ttf", "NanumGothic", "ttf"),
        ("/Library/Fonts/AppleGothic.ttf", "AppleGothic", "ttf"),
        (r"C:\\Windows\\Fonts\\malgun.ttf", "MalgunGothic", "ttf"),
        ("/usr/share/fonts/truetype/nanum/NanumGothic.ttf", "NanumGothic", "ttf"),
    ]
    for p, n, ext in sys_ttf:
        if os.path.exists(p):
            cands.append((n, p, ext))
    sys_ttc = [
        ("/System/Library/Fonts/AppleSDGothicNeo.ttc", "AppleSDGothicNeo", "ttc"),
        ("/usr/share/fonts/opentype/noto/NotoSansCJK-Regular.ttc", "NotoSansCJK", "ttc"),
        ("/usr/share/fonts/truetype/noto/NotoSansCJK-Regular.ttc", "NotoSansCJK", "ttc"),
    ]
    for p, n, ext in sys_ttc:
        if os.path.exists(p):
            cands.append((n, p, ext))
    uniq, seen = [], set()
    for name, path, ext in cands:
        key = (name, path)
        if key not in seen:
            seen.add(key)
            uniq.append((name, path, ext))
    return uniq

# ---- ReportLab ê²½ë¡œ (TTF/OTFë§Œ)

def _pdf_via_reportlab_from_md(md_text: str, title: str, font_name: str, font_path: str, ext: str):
    if ext not in ("ttf", "otf"):
        return None, "ReportLab ê²½ë¡œëŠ” TTF/OTFë§Œ ì§€ì› (TTC ê°ì§€)"
    try:
        from reportlab.lib.pagesizes import A4
        from reportlab.lib.units import mm
        from reportlab.platypus import SimpleDocTemplate, Paragraph, Spacer, Table, TableStyle, KeepTogether
        from reportlab.lib.styles import getSampleStyleSheet, ParagraphStyle
        from reportlab.pdfbase import pdfmetrics
        from reportlab.pdfbase.ttfonts import TTFont
        from reportlab.lib.enums import TA_LEFT
        from reportlab.lib import colors
    except Exception as e:
        return None, f"[reportlab ì„í¬íŠ¸ ì‹¤íŒ¨] {e}"

    try:
        pdfmetrics.registerFont(TTFont(font_name, font_path))
    except Exception as e:
        return None, f"TTF/OTF í°íŠ¸ ë“±ë¡ ì‹¤íŒ¨: {font_path} â€” {e}"

    styles = getSampleStyleSheet()
    base = ParagraphStyle(name="KBase", parent=styles["Normal"], fontName=font_name, fontSize=10.5, leading=14.5, alignment=TA_LEFT)
    h2 = ParagraphStyle(name="KH2", parent=base, fontSize=15, leading=19, spaceBefore=8, spaceAfter=6)
    h3 = ParagraphStyle(name="KH3", parent=base, fontSize=13, leading=17, spaceBefore=6, spaceAfter=4)
    code = ParagraphStyle(name="KCode", parent=base, fontName=font_name, fontSize=9.5, leading=13, backColor=colors.whitesmoke, borderPadding=(4,4,4,4))
    bullet = ParagraphStyle(name="KBullet", parent=base, leftIndent=12)

    def esc(s: str) -> str:
        return (s.replace("&","&amp;").replace("<","&lt;").replace(">","&gt;").replace("\t","    "))

    lines = (md_text or "").replace("\r\n","\n").split("\n")
    flow = []
    if title:
        flow.append(Paragraph(esc(title), h2)); flow.append(Spacer(1, 6))

    in_code = False
    code_buf, table_buf = [], []

    def flush_code():
        if code_buf:
            block = esc("\n".join(code_buf)).replace("  ", "&nbsp;&nbsp;").replace("\n","<br/>")
            flow.append(Paragraph(block, code))
            flow.append(Spacer(1, 4))
            code_buf.clear()

    def flush_table():
        if not table_buf:
            return
        rows = []
        for row in table_buf:
            cells = [c.strip() for c in row.strip("|").split("|")]
            rows.append(cells)
        tbl = Table(rows, hAlign='LEFT')
        tbl.setStyle(TableStyle([
            ('FONT', (0,0), (-1,-1), font_name, 9.5),
            ('INNERGRID', (0,0), (-1,-1), 0.25, colors.lightgrey),
            ('BOX', (0,0), (-1,-1), 0.5, colors.lightgrey),
            ('BACKGROUND', (0,0), (-1,0), colors.HexColor("#f1f5ff")),
            ('VALIGN', (0,0), (-1,-1), 'TOP'),
        ]))
        flow.append(KeepTogether(tbl))
        flow.append(Spacer(1, 4))
        table_buf.clear()

    for raw in lines:
        line = raw.rstrip()
        if line.strip().startswith("```"):
            if in_code:
                in_code = False
                flush_code()
            else:
                in_code = True
            continue
        if in_code:
            code_buf.append(line); continue

        if line.strip().startswith("|") and "|" in line.strip()[1:]:
            table_buf.append(line); continue
        else:
            flush_table()

        if line.startswith("## "):
            flow.append(Paragraph(esc(line[3:].strip()), h2)); flow.append(Spacer(1, 2)); continue
        if line.startswith("### "):
            flow.append(Paragraph(esc(line[4:].strip()), h3)); flow.append(Spacer(1, 2)); continue

        if re.match(r"^\s*[-*]\s+", line):
            flow.append(Paragraph("â€¢ " + esc(re.sub(r"^\s*[-*]\s+","",line)), bullet)); continue
        if re.match(r"^\s*\d+\.\s+", line):
            flow.append(Paragraph(esc(line), bullet)); continue

        if line.strip()=="":
            flow.append(Spacer(1,4))
        else:
            flow.append(Paragraph(esc(line), base))

    flush_code(); flush_table()

    try:
        from reportlab.lib.pagesizes import A4
        from reportlab.platypus import SimpleDocTemplate
        buffer = BytesIO()
        doc = SimpleDocTemplate(buffer, pagesize=A4,
                                leftMargin=18*mm, rightMargin=18*mm, topMargin=18*mm, bottomMargin=18*mm)
        doc.build(flow)
        buffer.seek(0)
        return buffer.read(), f"OK[ReportLab Markdown] font={font_name} ({font_path})"
    except Exception as e:
        return None, f"ReportLab PDF ì‹¤íŒ¨: {e}"

# ---- Pillow ê²½ë¡œ (TTC í¬í•¨)

def _pdf_via_pillow_text(text: str, title: str, font_path: str, ext: str):
    try:
        from PIL import Image, ImageDraw, ImageFont
    except Exception as e:
        return None, f"[Pillow ì„í¬íŠ¸ ì‹¤íŒ¨] {e}"

    DPI = 300
    A4_W, A4_H = int(8.27 * DPI), int(11.69 * DPI)
    L, R, T, B = int(0.6 * DPI), int(0.6 * DPI), int(0.7 * DPI), int(0.7 * DPI)
    MAX_W = A4_W - L - R

    font = None
    last_err = None
    if ext == "ttc":
        for idx in range(0, 8):
            try:
                font = ImageFont.truetype(font_path, size=22, index=idx)
                break
            except Exception as e:
                last_err = e
        if font is None:
            return None, f"Pillow í°íŠ¸ ë¡œë“œ ì‹¤íŒ¨(TTC): {last_err}"
    else:
        try:
            font = ImageFont.truetype(font_path, size=22)
        except Exception as e:
            return None, f"Pillow í°íŠ¸ ë¡œë“œ ì‹¤íŒ¨: {e}"

    line_gap = 10
    title_font = font.font_variant(size=28)

    def wrap(draw, text, f):
        words = text.split(" ")
        lines, cur = [], ""
        for w in words:
            test = (cur + " " + w).strip() if cur else w
            if draw.textlength(test, font=f) <= MAX_W:
                cur = test
            else:
                if cur: lines.append(cur)
                if draw.textlength(w, font=f) > MAX_W:
                    acc = ""
                    for ch in w:
                        if draw.textlength((acc+ch), font=f) <= MAX_W:
                            acc += ch
                        else:
                            lines.append(acc if acc else ch)
                            acc = ch
                    cur = acc
                else:
                    cur = w
        if cur: lines.append(cur)
        return lines

    blocks = []
    if title: blocks.append(("__TITLE__", title))
    for p in (text or "").replace("\r\n","\n").split("\n\n"):
        if p: blocks.append(("P", p))

    pages = []
    img = Image.new("L", (A4_W, A4_H), 255)
    draw = ImageDraw.Draw(img)
    y = T

    if blocks and blocks[0][0] == "__TITLE__":
        for ln in wrap(draw, blocks[0][1], title_font):
            draw.text((L, y), ln, font=title_font, fill=0); y += title_font.size + line_gap
        y += 10; blocks = blocks[1:]

    for _, para in blocks:
        lines = []
        for ln in para.split("\n"):
            lines.extend(wrap(draw, ln, font))
        hline = font.size + line_gap
        needed = len(lines) * hline + 8
        if y + needed > (A4_H - B):
            pages.append(img)
            img = Image.new("L", (A4_W, A4_H), 255)
            draw = ImageDraw.Draw(img)
            y = T
        for ln in lines:
            draw.text((L, y), ln, font=font, fill=0); y += hline
        y += 8
    pages.append(img)

    try:
        bio = BytesIO()
        pages[0].save(bio, format="PDF", save_all=True, append_images=pages[1:])
        bio.seek(0)
        return bio.read(), "OK[Pillow text]"
    except Exception as e:
        return None, f"Pillow PDF ì €ì¥ ì‹¤íŒ¨: {e}"


def markdown_to_pdf_korean(md_text: str, title: str|None=None):
    cands = _resolve_korean_font_candidates()
    if not cands:
        return None, "ì‚¬ìš©í•  í•œê¸€ í°íŠ¸ë¥¼ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤. ./fonts ì— TTF í•œ ê°œë§Œ ë„£ì–´ë‘ë©´ ê¹”ë”í•©ë‹ˆë‹¤."
    name, path, ext = cands[0]
    pdf, dbg = _pdf_via_reportlab_from_md(md_text or "", title or "", name, path, ext)
    if pdf:
        return pdf, dbg
    return _pdf_via_pillow_text(md_text or "", title or "", path, ext)


def text_to_pdf_bytes_korean(text: str, title: str|None=None):
    return markdown_to_pdf_korean(text, title=title)

# =========================
# PDF/HWPX/PDFí…ìŠ¤íŠ¸ ì¶”ì¶œ
# =========================

def extract_text_from_pdf(file_bytes: bytes) -> str:
    try:
        from PyPDF2 import PdfReader
        reader = PdfReader(BytesIO(file_bytes))
        texts = []
        for page in reader.pages:
            txt = page.extract_text() or ""
            texts.append(txt)
        return "\n".join(texts).strip()
    except Exception as e:
        return f"[PDF ì¶”ì¶œ ì‹¤íŒ¨] {e}"


def extract_text_from_hwpx_bytes(hwpx_bytes: bytes) -> str:
    """HWPX(ZIP) â†’ section XMLì—ì„œ í…ìŠ¤íŠ¸ ì¶”ì¶œ"""
    try:
        import xml.etree.ElementTree as ET
    except Exception as e:
        return f"[HWPX íŒŒì‹± ì‹¤íŒ¨] xml ëª¨ë“ˆ ì˜¤ë¥˜: {e}"

    try:
        buf = BytesIO(hwpx_bytes)
        with zipfile.ZipFile(buf, 'r') as zf:
            section_files = [n for n in zf.namelist() if n.startswith("Contents/") and n.lower().endswith(".xml")]
            section_files.sort()
            out_lines = []
            for name in section_files:
                try:
                    xml_data = zf.read(name)
                    root = ET.fromstring(xml_data)
                    text_parts = []
                    for elem in root.iter():
                        tag = elem.tag
                        if tag.endswith("t") or tag.endswith("para") or tag.endswith("run"):
                            if elem.text and elem.text.strip():
                                text_parts.append(elem.text.strip())
                    if text_parts:
                        out_lines.append("\n".join(text_parts))
                except Exception as e:
                    out_lines.append(f"[{name} íŒŒì‹± ê²½ê³ ] {e}")
            return "\n\n".join(out_lines).strip()
    except Exception as e:
        return f"[HWPX ZIP ì—´ê¸° ì‹¤íŒ¨] {e}"

# =========================
# HWP ë³€í™˜ íŒŒì´í”„ë¼ì¸ (ìš°ì„ ìˆœìœ„: hwp5txt â†’ unoconv)
# =========================

# --- 1) hwp5txt: .hwp â†’ í…ìŠ¤íŠ¸

def _run_hwp5txt_to_text(file_bytes: bytes, filename: str):
    tool = _which("hwp5txt")
    if not tool:
        return "", "hwp5txt ë¯¸ì„¤ì¹˜"
    in_fd, in_path = tempfile.mkstemp(suffix=".hwp"); os.close(in_fd)
    with open(in_path, "wb") as f:
        f.write(file_bytes)
    try:
        cp = subprocess.run([tool, in_path], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True, timeout=120)
        if cp.returncode == 0 and cp.stdout:
            txt = cp.stdout.strip()
            try: os.remove(in_path)
            except Exception: pass
            return txt, "OK[hwp5txt]"
        dbg = f"hwp5txt ì‹¤íŒ¨: {cp.stderr[:200]}"
        return "", dbg
    except subprocess.TimeoutExpired:
        return "", "hwp5txt íƒ€ì„ì•„ì›ƒ"
    except Exception as e:
        return "", f"hwp5txt ì˜¤ë¥˜: {e}"
    finally:
        try: os.remove(in_path)
        except Exception: pass

# --- 2) unoconv/soffice: .hwp â†’ PDF â†’ í…ìŠ¤íŠ¸


def _run_unoconv_hwp_to_pdf_bytes(file_bytes: bytes, filename: str):
    unoconv_bin = _which("unoconv")
    soffice_bin = _which("soffice") or _which("libreoffice")
    in_fd, in_path = tempfile.mkstemp(suffix=".hwp"); os.close(in_fd)
    with open(in_path, "wb") as f:
        f.write(file_bytes)

    out_dir = tempfile.mkdtemp()
    out_pdf = os.path.join(out_dir, os.path.splitext(os.path.basename(filename))[0] + ".pdf")

    try:
        if unoconv_bin:
            cp = subprocess.run([unoconv_bin, '-f', 'pdf', '-o', out_dir, in_path],
                                stdout=subprocess.PIPE, stderr=subprocess.PIPE, timeout=180)
            if cp.returncode != 0:
                raise RuntimeError(f"unoconv ì‹¤íŒ¨: {cp.stderr.decode(errors='ignore')[:200]}")
        elif soffice_bin:
            cp = subprocess.run([soffice_bin, '--headless', '--convert-to', 'pdf', '--outdir', out_dir, in_path],
                                stdout=subprocess.PIPE, stderr=subprocess.PIPE, timeout=180)
        else:
            raise FileNotFoundError("unoconv/soffice ë¯¸ì„¤ì¹˜")

        if not os.path.exists(out_pdf):
            cand = next((os.path.join(out_dir, fn) for fn in os.listdir(out_dir) if fn.lower().endswith('.pdf')), None)
            if not cand:
                raise FileNotFoundError("PDF ê²°ê³¼ íŒŒì¼ì„ ì°¾ì§€ ëª»í•¨ (LibreOfficeì—ì„œ HWP ë¯¸ì§€ì›ì¼ ìˆ˜ ìˆìŒ)")
            out_pdf = cand

        with open(out_pdf, 'rb') as rf:
            pdf_bytes = rf.read()
        return pdf_bytes, "OK[unoconv/sofficeâ†’PDF]"
    finally:
        try: os.remove(in_path)
        except Exception: pass
        try: shutil.rmtree(out_dir, ignore_errors=True)
        except Exception: pass


def _convert_hwp_priority(file_bytes: bytes, filename: str):
    """
    1) hwp5txt â†’ TXT ì§ì ‘ ì¶”ì¶œ (ìš°ì„ )
    2) unoconv/soffice â†’ PDF ë³€í™˜ í›„ PyPDF2 í…ìŠ¤íŠ¸ ì¶”ì¶œ (í´ë°±)
    """
    # 1) hwp5txt
    txt1, dbg1 = _run_hwp5txt_to_text(file_bytes, filename)
    if txt1:
        pdf_bytes1, dbg_pdf1 = text_to_pdf_bytes_korean(txt1, title=os.path.basename(filename) + " (hwp5txt ì¶”ì¶œ)")
        gen_pdf1 = (os.path.splitext(os.path.basename(filename))[0] + "_txt_extract.pdf", pdf_bytes1) if pdf_bytes1 else None
        return txt1, gen_pdf1, f"{dbg1} â†’ {dbg_pdf1}"

    # 2) unoconv/soffice
    try:
        pdf_bytes2, dbg2 = _run_unoconv_hwp_to_pdf_bytes(file_bytes, filename)
        if pdf_bytes2:
            txt2 = extract_text_from_pdf(pdf_bytes2)
            gen_pdf2 = (os.path.splitext(os.path.basename(filename))[0] + "_converted.pdf", pdf_bytes2)
            return txt2, gen_pdf2, f"{dbg2} â†’ PDFí…ìŠ¤íŠ¸ ì¶”ì¶œ"
    except Exception as e:
        dbg2 = f"unoconv/soffice ì‹¤íŒ¨: {e}"

    return "", None, f"hwp5txt ì‹¤íŒ¨ Â· {dbg1} / {dbg2 if 'dbg2' in locals() else 'unoconv ë¯¸ì‹œë„'}"

# =========================
# ì—…ë¡œë“œ ì†ŒìŠ¤ ì²˜ë¦¬ (HWP íŒŒì´í”„ë¼ì¸ í†µí•©)
# =========================

DOC_EXTS = {".doc",".docx",".hwp",".hwpx",".xls",".xlsx",".pdf",".txt",".md",".csv",".log"}


def handle_uploaded_source_files(uploaded_files):
    combined_texts, convert_logs, generated_pdfs = [], [], []
    for f in uploaded_files:
        name = f.name
        data = f.read()
        ext = os.path.splitext(name)[1].lower()

        if ext == ".pdf":
            txt = extract_text_from_pdf(data)
            convert_logs.append(f"ğŸ“„ {name}: PDF í…ìŠ¤íŠ¸ ì¶”ì¶œ ì™„ë£Œ ({len(txt)} chars)")
            combined_texts.append(f"\n\n===== [{name}] =====\n{_redact_secrets(txt)}\n")

        elif ext == ".hwpx":
            txt = extract_text_from_hwpx_bytes(data)
            pdf_bytes, dbg = text_to_pdf_bytes_korean(txt, title=os.path.splitext(name)[0] + " (HWPX ì¶”ì¶œ)")
            if pdf_bytes:
                generated_pdfs.append((f"{os.path.splitext(name)[0]}_hwpx_extract.pdf", pdf_bytes))
            convert_logs.append(f"ğŸ“ {name}: HWPX íŒŒì‹± ì™„ë£Œ â†’ {dbg}")
            combined_texts.append(f"\n\n===== [{name}] (HWPXâ†’TXT) =====\n{_redact_secrets(txt)}\n")

        elif ext == ".hwp":
            txt, gen_pdf, dbg = _convert_hwp_priority(data, name)
            if txt:
                combined_texts.append(f"\n\n===== [{name}] (HWPâ†’TXT) =====\n{_redact_secrets(txt)}\n")
            if gen_pdf and gen_pdf[1]:
                generated_pdfs.append(gen_pdf)
            convert_logs.append(f"ğŸ“ {name}: {dbg}")
            if not txt:
                convert_logs.append(f"ğŸ›‘ {name}: í…ìŠ¤íŠ¸ ì¶”ì¶œ ì‹¤íŒ¨")

        elif ext in [".txt", ".csv", ".md", ".log"]:
            txt = _decode_best_effort(data)
            combined_texts.append(f"\n\n===== [{name}] =====\n{_redact_secrets(txt)}\n")
            convert_logs.append(f"ğŸ—’ï¸ {name}: í…ìŠ¤íŠ¸ ë¡œë“œ ì™„ë£Œ (auto-decode)")

        elif ext in [".docx"]:
            try:
                import docx
                doc = docx.Document(BytesIO(data))
                txt = "\n".join([p.text for p in doc.paragraphs])
                combined_texts.append(f"\n\n===== [{name}] =====\n{_redact_secrets(txt)}\n")
                convert_logs.append(f"ğŸ“ {name}: DOCX í…ìŠ¤íŠ¸ ì¶”ì¶œ ì™„ë£Œ")
            except Exception as e:
                convert_logs.append(f"âš ï¸ {name}: DOCX í…ìŠ¤íŠ¸ ì¶”ì¶œ ì‹¤íŒ¨ - {e}")

        else:
            convert_logs.append(f"â„¹ï¸ {name}: ì§€ì›í•˜ì§€ ì•ŠëŠ” í˜•ì‹(ì›ë³¸ ê·¸ëŒ€ë¡œ ì°¸ì¡°)")
    return "\n".join(combined_texts).strip(), convert_logs, generated_pdfs


# -*- coding: utf-8 -*-
# =============================================================
# app_part2.py â€” UI, ì°¨íŠ¸, ì²¨ë¶€ ë§¤íŠ¸ë¦­ìŠ¤, ë¡œê·¸ì¸, GPT ë˜í¼ (1/2)
# (ì´ íŒŒì¼ì€ app_part1.py ì•„ë˜ì— ê·¸ëŒ€ë¡œ ì´ì–´ë¶™ì—¬ í•œ íŒŒì¼ë¡œ ì‚¬ìš© ê°€ëŠ¥í•©ë‹ˆë‹¤)
# =============================================================

import os as _os
import re as _re
from io import BytesIO as _BytesIO
from datetime import datetime as _dt

import streamlit as st
import pandas as _pd
import numpy as _np
import plotly.express as px

# ----- ì—¬ê¸°ì„œë¶€í„° app_part1.py ë‚´ ì‹¬ë³¼ì„ ì‚¬ìš©í•©ë‹ˆë‹¤ -----
# handle_uploaded_source_files, markdown_to_pdf_korean, text_to_pdf_bytes_korean, _redact_secrets, DOC_EXTS ë“±

# =========================
# OpenAI ë˜í¼ (í´ë°± ë£¨í”„)
# =========================

API_KEYS = []  # í™˜ê²½ë³€ìˆ˜ OPENAI_API_KEY ê¶Œì¥. ì—¬ê¸°ëŠ” ë¹„ì›Œë‘¡ë‹ˆë‹¤.

def call_gpt_with_fallback(messages, temperature=0.3, max_tokens=2000, model="gpt-4.1"):
    try:
        from openai import OpenAI
    except Exception:
        raise Exception("openai ë¯¸ì„¤ì¹˜: pip install openai")

    keys = []
    if _os.environ.get("OPENAI_API_KEY"):
        keys.append(_os.environ["OPENAI_API_KEY"])
    keys.extend([k for k in API_KEYS if k and not str(k).startswith("sk-REPLACE_")])

    if not keys:
        raise Exception("API í‚¤ê°€ ì—†ìŠµë‹ˆë‹¤. OPENAI_API_KEY í™˜ê²½ë³€ìˆ˜ë¥¼ ì„¤ì •í•˜ì„¸ìš”.")

    guardrail_system = {
        "role": "system",
        "content": (
            "ì•ˆì „ ê°€ë“œë ˆì¼ì„ ì¤€ìˆ˜í•˜ì„¸ìš”. ë¯¼ê°ì •ë³´/APIí‚¤ëŠ” ë…¸ì¶œ ê¸ˆì§€. "
            "ì™¸ë¶€ ì›¹ ë‹¤ìš´ë¡œë“œ/í¬ë¡¤ë§ì€ ìˆ˜í–‰í•˜ì§€ ë§ê³ , ì‚¬ìš©ìê°€ íŒŒì¼ ì—…ë¡œë“œí•œ ë°ì´í„°ë§Œ ë¶„ì„í•˜ì„¸ìš”."
        ),
    }

    safe_messages = [guardrail_system]
    for m in messages:
        safe_messages.append({"role": m["role"], "content": _redact_secrets(m.get("content",""))})

    errors = []
    for idx, k in enumerate(keys, 1):
        try:
            client = OpenAI(api_key=k)
            resp = client.chat.completions.create(
                model=model,
                messages=safe_messages,
                temperature=temperature,
                max_tokens=max_tokens,
            )
            return resp.choices[0].message.content, idx
        except Exception as e:
            errors.append(f"[í‚¤ {idx}] {str(e)[:200]}")
            continue
    raise Exception("ëª¨ë“  í‚¤ ì‹¤íŒ¨:\n" + "\n".join(errors))

# =========================
# ì‹œê°í™” ì»¬ëŸ¬ & ì—…ì²´ ì •ê·œí™”
# =========================

VENDOR_COLOR_MAP = {
    "ì—˜ì§€ìœ í”ŒëŸ¬ìŠ¤": "#FF1493",
    "ì¼€ì´í‹°": "#FF0000",
    "ì—ìŠ¤ì¼€ì´ë¸Œë¡œë“œë°´ë“œ": "#FFD700",
    "ì—ìŠ¤ì¼€ì´í…”ë ˆì½¤": "#1E90FF",
}
OTHER_SEQ = ["#2E8B57","#6B8E23","#556B2F","#8B4513","#A0522D","#CD853F","#228B22","#006400"]


def normalize_vendor(name: str) -> str:
    s = str(name) if _pd.notna(name) else ""
    if "ì—˜ì§€ìœ í”ŒëŸ¬ìŠ¤" in s or "LGìœ í”ŒëŸ¬ìŠ¤" in s or "LG U" in s.upper():
        return "ì—˜ì§€ìœ í”ŒëŸ¬ìŠ¤"
    if s.startswith("ì¼€ì´í‹°") or " KT" in s or s == "KT" or "ì£¼ì‹íšŒì‚¬ ì¼€ì´í‹°" in s:
        return "ì¼€ì´í‹°"
    if "ë¸Œë¡œë“œë°´ë“œ" in s or "SKë¸Œë¡œë“œë°´ë“œ" in s:
        return "ì—ìŠ¤ì¼€ì´ë¸Œë¡œë“œë°´ë“œ"
    if "í…”ë ˆì½¤" in s or "SKí…”ë ˆì½¤" in s:
        return "ì—ìŠ¤ì¼€ì´í…”ë ˆì½¤"
    return s or "ê¸°íƒ€"

# =========================
# ì²¨ë¶€ ë§í¬ ë§¤íŠ¸ë¦­ìŠ¤
# =========================

def build_attachment_matrix(df_like: _pd.DataFrame, title_col: str) -> _pd.DataFrame:
    if title_col not in df_like.columns:
        return _pd.DataFrame(columns=[title_col,"ë³¸ê³µê³ ë§í¬","ì œì•ˆìš”ì²­ì„œ","ê³µê³ ì„œ","ê³¼ì—…ì§€ì‹œì„œ","ê·œê²©ì„œ","ê¸°íƒ€"])
    buckets = {}
    def add_link(title, category, name, url):
        if title not in buckets:
            buckets[title] = {k:{} for k in ["ë³¸ê³µê³ ë§í¬","ì œì•ˆìš”ì²­ì„œ","ê³µê³ ì„œ","ê³¼ì—…ì§€ì‹œì„œ","ê·œê²©ì„œ","ê¸°íƒ€"]}
        if url not in buckets[title][category]:
            buckets[title][category][url] = name

    n_cols = df_like.shape[1]
    for _, row in df_like.iterrows():
        title = str(row.get(title_col, "")).strip()
        if not title:
            continue
        for j in range(1, n_cols):
            url_col = df_like.columns[j]
            name_col = df_like.columns[j-1]
            url_val = row.get(url_col, None)
            name_val = row.get(name_col, None)
            if _pd.isna(url_val):
                continue
            raw = str(url_val).strip()
            if _is_url(raw):
                urls = [raw]
            else:
                toks = [u.strip() for u in raw.replace("\n",";").split(";")]
                urls = [u for u in toks if _is_url(u)]
                if not urls:
                    continue
            name_base = "" if _pd.isna(name_val) else str(name_val).strip()
            name_tokens = [n.strip() for n in name_base.replace("\n",";").split(";")] if name_base else []
            for k, u in enumerate(urls):
                disp_name = name_tokens[k] if k < len(name_tokens) and name_tokens[k] else (name_base or _filename_from_url(u))
                low_name = (disp_name or "").lower() + " " + _filename_from_url(u).lower()
                if ("ì œì•ˆìš”ì²­ì„œ" in low_name) or ("rfp" in low_name):
                    add_link(title,"ì œì•ˆìš”ì²­ì„œ",disp_name,u)
                elif ("ê³µê³ ì„œ" in low_name) or ("ê³µê³ ë¬¸" in low_name):
                    add_link(title,"ê³µê³ ì„œ",disp_name,u)
                elif "ê³¼ì—…ì§€ì‹œì„œ" in low_name:
                    add_link(title,"ê³¼ì—…ì§€ì‹œì„œ",disp_name,u)
                elif ("ê·œê²©ì„œ" in low_name) or ("spec" in low_name):
                    add_link(title,"ê·œê²©ì„œ",disp_name,u)
                elif ("http://" in u) or ("https://" in u):
                    add_link(title,"ë³¸ê³µê³ ë§í¬",disp_name,u)
                else:
                    add_link(title,"ê¸°íƒ€",disp_name,u)

    def join_html(d):
        if not d: return ""
        return " | ".join([f"<a href='{url}' target='_blank' rel='nofollow noopener'>{name}</a>" for url, name in d.items()])

    rows = []
    for title, catmap in buckets.items():
        rows.append({
            title_col: title,
            "ë³¸ê³µê³ ë§í¬": join_html(catmap["ë³¸ê³µê³ ë§í¬"]),
            "ì œì•ˆìš”ì²­ì„œ": join_html(catmap["ì œì•ˆìš”ì²­ì„œ"]),
            "ê³µê³ ì„œ": join_html(catmap["ê³µê³ ì„œ"]),
            "ê³¼ì—…ì§€ì‹œì„œ": join_html(catmap["ê³¼ì—…ì§€ì‹œì„œ"]),
            "ê·œê²©ì„œ": join_html(catmap["ê·œê²©ì„œ"]),
            "ê¸°íƒ€": join_html(catmap["ê¸°íƒ€"]),
        })
    out_df = _pd.DataFrame(rows)
    out_df = out_df.sort_values(by=[title_col]).reset_index(drop=True)
    return out_df


def render_attachment_table_html(df_links: _pd.DataFrame, title_col: str,
                                 min_title_px: int = 360, wide_link_px: int = 440, narrow_px: int = 280) -> str:
    cols = [title_col,"ë³¸ê³µê³ ë§í¬","ì œì•ˆìš”ì²­ì„œ","ê³µê³ ì„œ","ê³¼ì—…ì§€ì‹œì„œ","ê·œê²©ì„œ","ê¸°íƒ€"]
    present_cols = [c for c in cols if c in df_links.columns]
    def _th(c):
        if c == title_col: return f'<th class="col-title">{c}</th>'
        elif c in ["ë³¸ê³µê³ ë§í¬","ì œì•ˆìš”ì²­ì„œ","ê³µê³ ì„œ"]: return f'<th class="col-linkwide">{c}</th>'
        else: return f'<th class="col-narrow">{c}</th>'
    def _td(c, v):
        cls = "col-title" if c == title_col else ("col-linkwide" if c in ["ë³¸ê³µê³ ë§í¬","ì œì•ˆìš”ì²­ì„œ","ê³µê³ ì„œ"] else "col-narrow")
        val = "" if _pd.isna(v) else str(v)
        return f'<td class="{cls}">{val}</td>'
    html = []
    html.append(f"""
<style>
.attach-table {{
  width: 100%; border-collapse: collapse; table-layout: fixed; word-wrap: break-word; font-size: 11px; line-height: 1.35;
}}
.attach-table th, .attach-table td {{ border: 1px solid #d0d7de; padding: 6px 8px; vertical-align: top; }}
.attach-table thead th {{ background: #0d6efd; color: white; font-weight: 700; font-size: 11px; position: sticky; top: 0; z-index: 1; }}
.attach-table td a {{ text-decoration: none; color: #0b5ed7; }}
.attach-table td a:hover {{ text-decoration: underline; }}
.attach-table th.col-title, .attach-table td.col-title {{ min-width: {min_title_px}px; width: {min_title_px}px; white-space: normal; }}
.attach-table th.col-linkwide, .attach-table td.col-linkwide {{ min-width: {wide_link_px}px; width: {wide_link_px}px; white-space: normal; }}
.attach-table th.col-narrow, .attach-table td.col-narrow {{ min-width: {narrow_px}px; width: {narrow_px}px; white-space: normal; }}
</style>
<table class="attach-table">
  <thead><tr>{''.join([_th(c) for c in present_cols])}</tr></thead>
  <tbody>
""")
    for _, r in df_links.iterrows():
        html.append("<tr>")
        for c in present_cols:
            html.append(_td(c, r.get(c, "")))
        html.append("</tr>")
    html.append("</tbody></table>")
    return "\n".join(html)


def render_attachment_cards_html(df_links: _pd.DataFrame, title_col: str) -> str:
    cat_cols = ["ë³¸ê³µê³ ë§í¬","ì œì•ˆìš”ì²­ì„œ","ê³µê³ ì„œ","ê³¼ì—…ì§€ì‹œì„œ","ê·œê²©ì„œ","ê¸°íƒ€"]
    present_cols = [c for c in cat_cols if c in df_links.columns]
    if title_col not in df_links.columns:
        return "<p>í‘œì‹œí•  ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.</p>"
    css = """
<style>
.attch-wrap { display: flex; flex-direction: column; gap: 14px; background: #eef6ff; padding: 8px; border-radius: 12px; }
.attch-card { border: 1px solid #cfe1ff; border-radius: 12px; padding: 12px 14px; background: #f4f9ff; box-shadow: 0 1px 3px rgba(13,110,253,0.05); }
.attch-title { font-weight: 700; margin-bottom: 8px; font-size: 13px; line-height: 1.4; word-break: break-word; color: #0b2e5b; }
.attch-grid { display: grid; grid-template-columns: repeat(auto-fit, minmax(220px, 1fr)); gap: 10px; }
.attch-box { border: 1px solid #cfe1ff; border-radius: 10px; overflow: hidden; background: #ffffff; }
.attch-box-header { background: #0d6efd; color: #fff; font-weight: 700; font-size: 11px; padding: 6px 8px; display: flex; align-items: center; justify-content: space-between; }
.badge { background: rgba(255,255,255,0.2); color: #fff; padding: 0 6px; border-radius: 999px; font-size: 10px; }
.attch-box-body { padding: 8px; font-size: 12px; line-height: 1.45; word-break: break-word; color: #0b2447; }
.attch-box-body a { color: #0b5ed7; text-decoration: none; }
.attch-box-body a:hover { text-decoration: underline; }
.attch-box-body details summary { cursor: pointer; font-weight: 600; list-style: none; outline: none; color: #0b2447; }
.attch-box-body details summary::-webkit-details-marker { display: none; }
.attch-box-body details summary:after { content: "â–¼"; font-size: 10px; margin-left: 6px; color: #0b2447; }
</style>
"""
    html = [css, '<div class="attch-wrap">']
    for _, r in df_links.iterrows():
        title = str(r.get(title_col, "") or "")
        html.append('<div class="attch-card">')
        html.append(f'<div class="attch-title">{title}</div>')
        html.append('<div class="attch-grid">')
        for col in present_cols:
            raw = str(r.get(col, "") or "").strip()
            if not raw:
                continue
            parts = [p.strip() for p in raw.split("|") if p.strip()]
            count = len(parts)
            if count <= 3:
                body_html = raw
            else:
                head = " | ".join(parts[:3])
                tail = " | ".join(parts[3:])
                body_html = head + f'<details style="margin-top:6px;"><summary>ë”ë³´ê¸° ({count-3})</summary>{tail}</details>'
            html.append('<div class="attch-box">')
            html.append(f'<div class="attch-box-header">{col} <span class="badge">{count}</span></div>')
            html.append(f'<div class="attch-box-body">{body_html}</div>')
            html.append('</div>')
        html.append('</div></div>')
    html.append("</div>")
    return "\n".join(html)

# =========================
# ì°¨íŠ¸ ë Œë”
# =========================

def render_basic_analysis_charts(base_df: _pd.DataFrame):
    def pick_unit(max_val: float):
        if max_val >= 1_0000_0000_0000: return ("ì¡°ì›", 1_0000_0000_0000)
        elif max_val >= 100_000_000: return ("ì–µì›", 100_000_000)
        elif max_val >= 1_000_000: return ("ë°±ë§Œì›", 1_000_000)
        else: return ("ì›", 1)
    def apply_unit(values: _pd.Series, mode: str = "ìë™"):
        unit_map = {"ì›":("ì›",1),"ë°±ë§Œì›":("ë°±ë§Œì›",1_000_000),"ì–µì›":("ì–µì›",100_000_000),"ì¡°ì›":("ì¡°ì›",1_0000_0000_0000)}
        if mode == "ìë™":
            u, f = pick_unit(values.max() if len(values) else 0); return values / f, u
        else:
            u, f = unit_map.get(mode, ("ì›",1)); return values / f, u

    st.markdown("## ğŸ“Š ê¸°ë³¸ í†µê³„ ë¶„ì„")
    st.caption("â€» ì´í•˜ ëª¨ë“  ì°¨íŠ¸ëŠ” **ë‚™ì°°ìì„ ì •ì—¬ë¶€ == 'Y'** ê¸°ì¤€ìœ¼ë¡œ ì‘ì„±ë©ë‹ˆë‹¤.")

    if "ë‚™ì°°ìì„ ì •ì—¬ë¶€" not in base_df.columns:
        st.warning("ì»¬ëŸ¼ 'ë‚™ì°°ìì„ ì •ì—¬ë¶€'ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."); return

    dwin = base_df[base_df["ë‚™ì°°ìì„ ì •ì—¬ë¶€"] == "Y"].copy()
    if dwin.empty:
        st.warning("ë‚™ì°°(Y) ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤."); return

    for col in ["íˆ¬ì°°ê¸ˆì•¡","ë°°ì •ì˜ˆì‚°ê¸ˆì•¡","íˆ¬ì°°ìœ¨"]:
        if col in dwin.columns:
            dwin[col] = _pd.to_numeric(dwin[col], errors="coerce")

    if "ëŒ€í‘œì—…ì²´" in dwin.columns:
        dwin["ëŒ€í‘œì—…ì²´_í‘œì‹œ"] = dwin["ëŒ€í‘œì—…ì²´"].map(normalize_vendor)
    else:
        dwin["ëŒ€í‘œì—…ì²´_í‘œì‹œ"] = "ê¸°íƒ€"

    st.markdown("### 1) ëŒ€í‘œì—…ì²´ë³„ ë¶„í¬")
    unit_choice = st.selectbox("íŒŒì´ì°¨íŠ¸(íˆ¬ì°°ê¸ˆì•¡ í•©ê³„) í‘œê¸° ë‹¨ìœ„", ["ìë™","ì›","ë°±ë§Œì›","ì–µì›","ì¡°ì›"], index=0)
    col_pie1, col_pie2 = st.columns(2)

    with col_pie1:
        if "íˆ¬ì°°ê¸ˆì•¡" in dwin.columns:
            sum_by_company = dwin.groupby("ëŒ€í‘œì—…ì²´_í‘œì‹œ")["íˆ¬ì°°ê¸ˆì•¡"].sum().reset_index().sort_values("íˆ¬ì°°ê¸ˆì•¡", ascending=False)
            scaled_vals, unit_label = apply_unit(sum_by_company["íˆ¬ì°°ê¸ˆì•¡"].fillna(0), unit_choice)
            sum_by_company["í‘œì‹œê¸ˆì•¡"] = scaled_vals
            fig1 = px.pie(sum_by_company, names="ëŒ€í‘œì—…ì²´_í‘œì‹œ", values="í‘œì‹œê¸ˆì•¡",
                          title=f"ëŒ€í‘œì—…ì²´ë³„ íˆ¬ì°°ê¸ˆì•¡ í•©ê³„ â€” ë‹¨ìœ„: {unit_label}",
                          color="ëŒ€í‘œì—…ì²´_í‘œì‹œ", color_discrete_map=VENDOR_COLOR_MAP, color_discrete_sequence=OTHER_SEQ)
            fig1.update_traces(
                hovertemplate="<b>%{label}</b><br>ê¸ˆì•¡: %{value:,.2f} " + unit_label + "<br>ë¹„ì¤‘: %{percent}",
                texttemplate="%{label}<br>%{value:,.2f} " + unit_label, textposition="auto")
            st.plotly_chart(fig1, use_container_width=True)
        else:
            st.info("íˆ¬ì°°ê¸ˆì•¡ ì»¬ëŸ¼ì´ ì—†ì–´ íŒŒì´ì°¨íŠ¸(ê¸ˆì•¡)ë¥¼ ê±´ë„ˆëœë‹ˆë‹¤.")

    with col_pie2:
        cnt_by_company = dwin["ëŒ€í‘œì—…ì²´_í‘œì‹œ"].value_counts().reset_index()
        cnt_by_company.columns = ["ëŒ€í‘œì—…ì²´_í‘œì‹œ","ê±´ìˆ˜"]
        fig2 = px.pie(cnt_by_company, names="ëŒ€í‘œì—…ì²´_í‘œì‹œ", values="ê±´ìˆ˜",
                      title="ëŒ€í‘œì—…ì²´ë³„ ë‚™ì°° ê±´ìˆ˜",
                      color="ëŒ€í‘œì—…ì²´_í‘œì‹œ", color_discrete_map=VENDOR_COLOR_MAP, color_discrete_sequence=OTHER_SEQ)
        fig2.update_traces(hovertemplate="<b>%{label}</b><br>ê±´ìˆ˜: %{value:,}ê±´<br>ë¹„ì¤‘: %{percent}",
                           texttemplate="%{label}<br>%{value:,}ê±´", textposition="auto")
        st.plotly_chart(fig2, use_container_width=True)

    st.markdown("### 2) íˆ¬ì°°ìœ¨ ì‚°ì ë„  &  3) ì—…ì²´/ë…„ë„ë³„ ìˆ˜ì£¼ê¸ˆì•¡")
    col_scatter, col_bar3 = st.columns(2)
    with col_scatter:
        if "íˆ¬ì°°ìœ¨" in dwin.columns:
            dwin["ê³µê³ ê²Œì‹œì¼ì_date"] = _pd.to_datetime(dwin.get("ê³µê³ ê²Œì‹œì¼ì_date", _pd.NaT), errors="coerce")
            dplot = dwin.dropna(subset=["íˆ¬ì°°ìœ¨", "ê³µê³ ê²Œì‹œì¼ì_date"]).copy()
            dplot = dplot[dplot["íˆ¬ì°°ìœ¨"] <= 300]
            hover_cols = [c for c in ["ëŒ€í‘œì—…ì²´_í‘œì‹œ","ìˆ˜ìš”ê¸°ê´€ëª…","ê³µê³ ëª…","ì…ì°°ê³µê³ ëª…","ì…ì°°ê³µê³ ë²ˆí˜¸"] if c in dplot.columns]
            fig_scatter = px.scatter(dplot, x="ê³µê³ ê²Œì‹œì¼ì_date", y="íˆ¬ì°°ìœ¨",
                                     hover_data=hover_cols, title="íˆ¬ì°°ìœ¨ ì‚°ì ë„",
                                     color="ëŒ€í‘œì—…ì²´_í‘œì‹œ", color_discrete_map=VENDOR_COLOR_MAP, color_discrete_sequence=OTHER_SEQ)
            st.plotly_chart(fig_scatter, use_container_width=True)
        else:
            st.info("íˆ¬ì°°ìœ¨ ì»¬ëŸ¼ ì—†ìŒ - ì‚°ì ë„ ìƒëµ")

    with col_bar3:
        if "íˆ¬ì°°ê¸ˆì•¡" in dwin.columns:
            dyear = dwin.copy()
            dyear["ì—°ë„"] = _pd.to_datetime(dyear.get("ê³µê³ ê²Œì‹œì¼ì_date", _pd.NaT), errors="coerce").dt.year
            dyear = dyear.dropna(subset=["ì—°ë„"])
            by_vendor_year = dyear.groupby(["ì—°ë„","ëŒ€í‘œì—…ì²´_í‘œì‹œ"])["íˆ¬ì°°ê¸ˆì•¡"].sum().reset_index()
            fig_vy = px.bar(by_vendor_year, x="ì—°ë„", y="íˆ¬ì°°ê¸ˆì•¡", color="ëŒ€í‘œì—…ì²´_í‘œì‹œ",
                            barmode="group", title="ì—…ì²´/ë…„ë„ë³„ ìˆ˜ì£¼ê¸ˆì•¡",
                            color_discrete_map=VENDOR_COLOR_MAP, color_discrete_sequence=OTHER_SEQ)
            fig_vy.update_traces(hovertemplate="<b>%{x}ë…„</b><br>%{legendgroup}: %{y:,.0f} ì›")
            st.plotly_chart(fig_vy, use_container_width=True)
        else:
            st.info("íˆ¬ì°°ê¸ˆì•¡ ì»¬ëŸ¼ì´ ì—†ì–´ 'ì—…ì²´/ë…„ë„ë³„ ìˆ˜ì£¼ê¸ˆì•¡'ì„ í‘œì‹œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")

    st.markdown("### 4) ì—°Â·ë¶„ê¸°ë³„ ë°°ì •ì˜ˆì‚°ê¸ˆì•¡ â€” ëˆ„ì  ë§‰ëŒ€ & ì´í•©")
    col_stack, col_total = st.columns(2)

    if "ë°°ì •ì˜ˆì‚°ê¸ˆì•¡" not in dwin.columns:
        with col_stack:
            st.info("ë°°ì •ì˜ˆì‚°ê¸ˆì•¡ ì»¬ëŸ¼ ì—†ìŒ - ë§‰ëŒ€ê·¸ë˜í”„ ìƒëµ")
        return

    dwin["ê³µê³ ê²Œì‹œì¼ì_date"] = _pd.to_datetime(dwin.get("ê³µê³ ê²Œì‹œì¼ì_date", _pd.NaT), errors="coerce")
    g = dwin.dropna(subset=["ê³µê³ ê²Œì‹œì¼ì_date"]).copy()
    if g.empty:
        with col_stack: st.info("ìœ íš¨í•œ ë‚ ì§œê°€ ì—†ì–´ ê·¸ë˜í”„ë¥¼ í‘œì‹œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        return

    g["ì—°ë„"] = g["ê³µê³ ê²Œì‹œì¼ì_date"].dt.year
    g["ë¶„ê¸°"] = g["ê³µê³ ê²Œì‹œì¼ì_date"].dt.quarter
    g["ì—°ë„ë¶„ê¸°"] = g["ì—°ë„"].astype(str) + " Q" + g["ë¶„ê¸°"].astype(str)
    if "ëŒ€í‘œì—…ì²´_í‘œì‹œ" not in g.columns:
        g["ëŒ€í‘œì—…ì²´_í‘œì‹œ"] = g.get("ëŒ€í‘œì—…ì²´", _pd.Series([""]*len(g))).map(normalize_vendor)

    title_col = "ì…ì°°ê³µê³ ëª…" if "ì…ì°°ê³µê³ ëª…" in g.columns else ("ê³µê³ ëª…" if "ê³µê³ ëª…" in g.columns else None)
    group_col = "ëŒ€í‘œì—…ì²´_í‘œì‹œ"
    if group_col not in g.columns:
        with col_stack:
            st.info("ëŒ€í‘œì—…ì²´_í‘œì‹œ ì»¬ëŸ¼ì´ ì—†ì–´ ëˆ„ì  ë§‰ëŒ€ë¥¼ í‘œì‹œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        return

    with col_stack:
        grp = (
            g.groupby(["ì—°ë„ë¶„ê¸°", group_col])["ë°°ì •ì˜ˆì‚°ê¸ˆì•¡"]
            .sum().reset_index(name="ê¸ˆì•¡í•©")
        )
        if not grp.empty:
            if title_col:
                title_map = (
                    g.groupby(["ì—°ë„ë¶„ê¸°", group_col])[title_col]
                    .apply(lambda s: " | ".join(_pd.Series(s).dropna().astype(str).unique()[:10]))
                    .rename("ì…ì°°ê³µê³ ëª©ë¡").reset_index()
                )
                grp = grp.merge(title_map, on=["ì—°ë„ë¶„ê¸°", group_col], how="left")
                grp["ì…ì°°ê³µê³ ëª©ë¡"] = grp["ì…ì°°ê³µê³ ëª©ë¡"].fillna("")
            else:
                grp["ì…ì°°ê³µê³ ëª©ë¡"] = ""

            grp["ì—°"] = grp["ì—°ë„ë¶„ê¸°"].str.extract(r"(\d{4})").astype(int)
            grp["ë¶„"] = grp["ì—°ë„ë¶„ê¸°"].str.extract(r"Q(\d)").astype(int)
            grp = grp.sort_values(["ì—°","ë¶„",group_col]).reset_index(drop=True)
            ordered_quarters = grp.sort_values(["ì—°","ë¶„"])["ì—°ë„ë¶„ê¸°"].unique()
            grp["ì—°ë„ë¶„ê¸°"] = _pd.Categorical(grp["ì—°ë„ë¶„ê¸°"], categories=ordered_quarters, ordered=True)

            custom = _np.column_stack([grp[group_col].astype(str).to_numpy(),
                                      grp["ì…ì°°ê³µê³ ëª©ë¡"].astype(str).to_numpy()])

            fig_stack = px.bar(
                grp, x="ì—°ë„ë¶„ê¸°", y="ê¸ˆì•¡í•©",
                color=group_col, barmode="stack",
                title=f"ì—°Â·ë¶„ê¸°ë³„ ë°°ì •ì˜ˆì‚°ê¸ˆì•¡ â€” ëˆ„ì (ìŠ¤íƒ) / ê·¸ë£¹: {group_col}",
                color_discrete_map=VENDOR_COLOR_MAP, color_discrete_sequence=OTHER_SEQ,
            )
            fig_stack.update_traces(
                customdata=custom,
                hovertemplate=(
                    "<b>%{x}</b><br>"
                    f"{group_col}: %{{customdata[0]}}<br>"
                    "ê¸ˆì•¡: %{{y:,.0f}} ì›<br>"
                    "ì…ì°°ê³µê³ ëª…: %{{customdata[1]}}"
                )
            )
            fig_stack.update_layout(xaxis_title="ì—°ë„ë¶„ê¸°", yaxis_title="ë°°ì •ì˜ˆì‚°ê¸ˆì•¡ (ì›)",
                                    margin=dict(l=10,r=10,t=60,b=10))
            st.plotly_chart(fig_stack, use_container_width=True)
        else:
            st.info("ê·¸ë£¹í•‘ ê²°ê³¼ê°€ ë¹„ì–´ ìˆìŠµë‹ˆë‹¤.")

    with col_total:
        grp_total = g.groupby("ì—°ë„ë¶„ê¸°")["ë°°ì •ì˜ˆì‚°ê¸ˆì•¡"].sum().reset_index(name="ê¸ˆì•¡í•©")
        grp_total["ì—°"] = grp_total["ì—°ë„ë¶„ê¸°"].str.extract(r"(\d{4})").astype(int)
        grp_total["ë¶„"] = grp_total["ì—°ë„ë¶„ê¸°"].str.extract(r"Q(\d)").astype(int)
        grp_total = grp_total.sort_values(["ì—°","ë¶„"])
        if title_col:
            titles_total = g.groupby("ì—°ë„ë¶„ê¸°")[title_col].apply(
                lambda s: " | ".join(_pd.Series(s).dropna().astype(str).unique()[:10])
            ).reindex(grp_total["ì—°ë„ë¶„ê¸°"]).fillna("")
            custom2 = _np.stack([titles_total], axis=-1)
        else:
            custom2 = _np.stack([_pd.Series([""])], axis=-1)
        fig_bar = px.bar(grp_total, x="ì—°ë„ë¶„ê¸°", y="ê¸ˆì•¡í•©", title="ì—°Â·ë¶„ê¸°ë³„ ë°°ì •ì˜ˆì‚°ê¸ˆì•¡ (ì´í•©)", text="ê¸ˆì•¡í•©")
        fig_bar.update_traces(
            customdata=custom2,
            hovertemplate="<b>%{x}</b><br>ì´ì•¡: %{y:,.0f} ì›<br>ì…ì°°ê³µê³ ëª…: %{customdata[0]}",
            texttemplate='%{text:,.0f}', textposition='outside', cliponaxis=False
        )
        st.plotly_chart(fig_bar, use_container_width=True)

# -*- coding: utf-8 -*-
# =============================================================
# app_part2.py â€” UI, ì°¨íŠ¸, ì²¨ë¶€ ë§¤íŠ¸ë¦­ìŠ¤, ë¡œê·¸ì¸, GPT ë˜í¼ (2/2)
# (ì´ íŒŒì¼ì€ app_part1.py ë° app_part2.py(1/2) ì•„ë˜ì— ê·¸ëŒ€ë¡œ ì´ì–´ë¶™ì—¬ í•œ íŒŒì¼ë¡œ ì‚¬ìš© ê°€ëŠ¥í•©ë‹ˆë‹¤)
# =============================================================

import os as _os
import re as _re
from io import BytesIO as _BytesIO
from datetime import datetime as _dt

import streamlit as st
import pandas as _pd
import numpy as _np
import plotly.express as px

# =========================
# ë¡œê·¸ì¸ ê²Œì´íŠ¸
# =========================

def login_gate():
    import streamlit.components.v1 as components
    components.html(
        """
        <script>
        (function(){
          if (!window.__warned__) {
            alert("ë³¸ ì‚¬ì´íŠ¸ëŠ” All Rights Reservedì´ë©°, í—ˆê°€ë°›ì§€ ì•Šì€ ì‚¬ìš©ìëŠ” ì‚¬ìš© ì‹œ ë²•ì ì¸ ì±…ì„ì„ ì§‘ë‹ˆë‹¤.");
            window.__warned__ = true;
          }
        })();
        </script>
        """,
        height=0,
    )
    st.title("ğŸ” ë¡œê·¸ì¸")
    emp = st.text_input("ì‚¬ë²ˆ", value="", placeholder="ì˜ˆ: 9999")
    dob = st.text_input("ìƒë…„ì›”ì¼(YYMMDD)", value="", placeholder="ì˜ˆ: 990101")
    col1, col2 = st.columns([1,1])
    with col1:
        if st.button("ë¡œê·¸ì¸", type="primary", use_container_width=True):
            if emp == "2855" and dob == "910518":  # ë°ëª¨ ìê²©
                st.session_state["authed"] = True
                st.success("ë¡œê·¸ì¸ ì„±ê³µ")
                st.rerun()
            else:
                st.error("ì¸ì¦ ì‹¤íŒ¨. ì‚¬ë²ˆ/ìƒë…„ì›”ì¼ì„ í™•ì¸í•˜ì„¸ìš”.")
    with col2:
        st.info("SKë¸Œë¡œë“œë°´ë“œ ì‚¬ë²ˆ 4ìë¦¬ì™€ ìƒë…„ì›”ì¼ 6ìë¦¬ë¥¼ ì…ë ¥í•˜ì„¸ìš”. ë¬¸ì˜ : 8girim@sk.com")


# =========================
# ë©”ì¸ ì•±
# =========================

def main():
    # ì„¸ì…˜ ìƒíƒœ
    if "gpt_report_md" not in st.session_state:
        st.session_state["gpt_report_md"] = None
    if "generated_src_pdfs" not in st.session_state:
        st.session_state["generated_src_pdfs"] = []
    if "authed" not in st.session_state:
        st.session_state["authed"] = False
    if "chat_messages" not in st.session_state:
        st.session_state["chat_messages"] = []

    AUThed = st.session_state.get("authed", False)

    # ì‚¬ì´ë“œë°”
    st.sidebar.title("ğŸ“‚ ë°ì´í„° ì—…ë¡œë“œ")
    uploaded_file = st.sidebar.file_uploader(
        "filtered ì‹œíŠ¸ê°€ í¬í•¨ëœ ë³‘í•© ì—‘ì…€ ì—…ë¡œë“œ (.xlsx)", type=["xlsx"], disabled=not AUThed
    )
    menu = st.sidebar.radio("# ğŸ“‹ ë©”ë‰´ ì„ íƒ", ["ì¡°ë‹¬ì…ì°°ê²°ê³¼í˜„í™©","ë‚´ê³ ê° ë¶„ì„í•˜ê¸°"], disabled=not AUThed)

    # GPT ìƒíƒœ
    _ok = bool(_os.environ.get("OPENAI_API_KEY"))
    st.sidebar.success("GPT ì‚¬ìš© ê°€ëŠ¥" if _ok else "GPT ë²„íŠ¼ í™œì„± (í™˜ê²½ë³€ìˆ˜ í•„ìš”)")

    gpt_extra_req = st.sidebar.text_area("ğŸ¤– GPT ì¶”ê°€ ìš”êµ¬ì‚¬í•­(ì„ íƒ)", height=120,
                                         placeholder="ì˜ˆ) 'MACsec, SRv6 ê°•ì¡°', 'ì„¸ë¶€ ì¼ì • í‘œ ì¶”ê°€' ë“±",
                                         disabled=not AUThed)

    if not AUThed:
        login_gate()
        st.stop()

    if not uploaded_file:
        st.title("ğŸ“Š ì¡°ë‹¬ì…ì°° ë¶„ì„ ì‹œìŠ¤í…œ")
        st.info("ì¢Œì¸¡ì—ì„œ 'filtered' ì‹œíŠ¸ë¥¼ í¬í•¨í•œ ì—‘ì…€ íŒŒì¼ì„ ì—…ë¡œë“œí•˜ì„¸ìš”.")
        st.stop()

    # ì—‘ì…€ ë¡œë“œ
    df = _pd.read_excel(uploaded_file, sheet_name="filtered", engine="openpyxl")
    df_original = df.copy()

    # í•„í„°
    st.sidebar.markdown(" ğŸ” ë°ì´í„° ì»¬ëŸ¼ í™•ì¸")
    if st.sidebar.checkbox("ì»¬ëŸ¼ëª… ë³´ê¸°", value=False):
        st.sidebar.write(df.columns.tolist())

    only_winner = st.sidebar.checkbox("(í•„í„°)ë‚™ì°°ìì„ ì •ì—¬ë¶€ = 'Y' ë§Œ ë³´ê¸°", value=True)

    if "ëŒ€í‘œì—…ì²´" in df.columns:
        company_list = sorted(df["ëŒ€í‘œì—…ì²´"].dropna().unique())
        selected_companies = st.sidebar.multiselect("ëŒ€í‘œì—…ì²´ í•„í„° (ë³µìˆ˜ ê°€ëŠ¥)", company_list)
    else:
        selected_companies = []

    demand_col_sidebar = "ìˆ˜ìš”ê¸°ê´€ëª…" if "ìˆ˜ìš”ê¸°ê´€ëª…" in df.columns else ("ìˆ˜ìš”ê¸°ê´€" if "ìˆ˜ìš”ê¸°ê´€" in df.columns else None)
    if demand_col_sidebar:
        org_list = sorted(df[demand_col_sidebar].dropna().unique())
        selected_orgs = st.sidebar.multiselect(f"{demand_col_sidebar} í•„í„° (ë³µìˆ˜ ê°€ëŠ¥)", org_list)
    else:
        selected_orgs = []

    st.sidebar.subheader("ğŸ“† ê³µê³ ê²Œì‹œì¼ì í•„í„°")
    if "ê³µê³ ê²Œì‹œì¼ì_date" in df.columns:
        df["ê³µê³ ê²Œì‹œì¼ì_date"] = _pd.to_datetime(df["ê³µê³ ê²Œì‹œì¼ì_date"], errors="coerce")
    else:
        df["ê³µê³ ê²Œì‹œì¼ì_date"] = _pd.NaT
    df["year"] = df["ê³µê³ ê²Œì‹œì¼ì_date"].dt.year
    df["month"] = df["ê³µê³ ê²Œì‹œì¼ì_date"].dt.month
    year_list = sorted([int(x) for x in df["year"].dropna().unique()])
    selected_years = st.sidebar.multiselect("ì—°ë„ ì„ íƒ (ë³µìˆ˜ ê°€ëŠ¥)", year_list, default=[])
    month_list = list(range(1,13))
    selected_months = st.sidebar.multiselect("ì›” ì„ íƒ (ë³µìˆ˜ ê°€ëŠ¥)", month_list, default=[])
    st.sidebar.markdown("---")

    df_filtered = df.copy()
    if selected_years: df_filtered = df_filtered[df_filtered["year"].isin(selected_years)]
    if selected_months: df_filtered = df_filtered[df_filtered["month"].isin(selected_months)]
    if only_winner and "ë‚™ì°°ìì„ ì •ì—¬ë¶€" in df_filtered.columns:
        df_filtered = df_filtered[df_filtered["ë‚™ì°°ìì„ ì •ì—¬ë¶€"] == "Y"]
    if selected_companies and "ëŒ€í‘œì—…ì²´" in df_filtered.columns:
        df_filtered = df_filtered[df_filtered["ëŒ€í‘œì—…ì²´"].isin(selected_companies)]
    if selected_orgs and demand_col_sidebar:
        df_filtered = df_filtered[df_filtered[demand_col_sidebar].isin(selected_orgs)]

    # í˜ì´ì§€ ë¶„ê¸°
    if menu == "ì¡°ë‹¬ì…ì°°ê²°ê³¼í˜„í™©":
        st.title("ğŸ“‘ ì¡°ë‹¬ì…ì°°ê²°ê³¼í˜„í™©")

        dl_buf = _BytesIO()
        df_filtered.to_excel(dl_buf, index=False, engine="openpyxl"); dl_buf.seek(0)
        st.download_button(
            label="ğŸ“¥ í•„í„°ë§ëœ ë°ì´í„° ë‹¤ìš´ë¡œë“œ (Excel)",
            data=dl_buf,
            file_name=f"filtered_result_{_dt.now().strftime('%Y%m%d_%H%M')}.xlsx",
            mime="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet"
        )

        st.data_editor(df_filtered, use_container_width=True, key="result_editor", height=520)

        with st.expander("ğŸ“Š ê¸°ë³¸ í†µê³„ ë¶„ì„(ì°¨íŠ¸) ì—´ê¸°", expanded=False):
            render_basic_analysis_charts(df_filtered)

    else:
        st.title("ğŸ§‘â€ğŸ’¼ ë‚´ê³ ê° ë¶„ì„í•˜ê¸°")
        st.info("â„¹ï¸ ì´ ë©”ë‰´ëŠ” ì‚¬ì´ë“œë°” í•„í„°ì™€ ë¬´ê´€í•˜ê²Œ **ì „ì²´ ì›ë³¸ ë°ì´í„°**ë¥¼ ëŒ€ìƒìœ¼ë¡œ ê²€ìƒ‰í•©ë‹ˆë‹¤.")

        demand_col = None
        for col in ["ìˆ˜ìš”ê¸°ê´€ëª…","ìˆ˜ìš”ê¸°ê´€","ê¸°ê´€ëª…"]:
            if col in df_original.columns:
                demand_col = col; break
        if not demand_col:
            st.error("âš ï¸ ìˆ˜ìš”ê¸°ê´€ ê´€ë ¨ ì»¬ëŸ¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."); st.stop()
        st.success(f"âœ… ê²€ìƒ‰ ëŒ€ìƒ ì»¬ëŸ¼: **{demand_col}**")

        customer_input = st.text_input(f"ê³ ê°ì‚¬ëª…ì„ ì…ë ¥í•˜ì„¸ìš” ({demand_col} ê¸°ì¤€, ì‰¼í‘œë¡œ ë³µìˆ˜ ì…ë ¥ ê°€ëŠ¥)", help="ì˜ˆ) ì¡°ë‹¬ì²­, êµ­ë°©ë¶€")

        with st.expander(f"ğŸ“‹ ì „ì²´ {demand_col} ëª©ë¡ ë³´ê¸° (ê²€ìƒ‰ ì°¸ê³ ìš©)"):
            unique_orgs = sorted(df_original[demand_col].dropna().unique())
            st.write(f"ì´ {len(unique_orgs)}ê°œ ê¸°ê´€")
            search_org = st.text_input("ê¸°ê´€ëª… ê²€ìƒ‰", key="search_org_in_my")
            view_orgs = [o for o in unique_orgs if (search_org in str(o))] if search_org else unique_orgs
            st.write(view_orgs[:120])

        if customer_input:
            customers = [c.strip() for c in customer_input.split(",") if c.strip()]
            if customers:
                result = df_original[df_original[demand_col].isin(customers)]
                st.subheader(f"ğŸ“Š ê²€ìƒ‰ ê²°ê³¼: {len(result)}ê±´")
                if not result.empty:
                    rb = _BytesIO()
                    result.to_excel(rb, index=False, engine="openpyxl"); rb.seek(0)
                    st.download_button(
                        label="ğŸ“¥ ê²°ê³¼ ë°ì´í„° ë‹¤ìš´ë¡œë“œ (Excel)",
                        data=rb,
                        file_name=f"{'_'.join(customers)}_ì´ë ¥_{_dt.now().strftime('%Y%m%d_%H%M')}.xlsx",
                        mime="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet"
                    )

                    st.data_editor(result, use_container_width=True, key="customer_editor", height=520)

                    st.markdown("---")
                    st.subheader("ğŸ”— ì…ì°°ê³µê³ ëª… ê¸°ì¤€ìœ¼ë¡œ URLì„ ë¶„ë¥˜í•©ë‹ˆë‹¤.")
                    st.caption("(ë³¸ê³µê³ ë§í¬/ì œì•ˆìš”ì²­ì„œ/ê³µê³ ì„œ/ê³¼ì—…ì§€ì‹œì„œ/ê·œê²©ì„œ/ê¸°íƒ€, URL ì¤‘ë³µ ì œê±°)")

                    title_col_candidates = ["ì…ì°°ê³µê³ ëª…","ê³µê³ ëª…"]
                    title_col = next((c for c in title_col_candidates if c in result.columns), None)

                    if not title_col:
                        st.error("âš ï¸ 'ì…ì°°ê³µê³ ëª…' ë˜ëŠ” 'ê³µê³ ëª…' ì»¬ëŸ¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                    else:
                        attach_df = build_attachment_matrix(result, title_col)
                        if attach_df.empty:
                            st.info("ë¶„ë¥˜í•  ìˆ˜ ìˆëŠ” ë§í¬ë¥¼ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")
                        else:
                            use_compact = st.toggle("ğŸ”€ ê·¸ë£¹í˜•(Compact) ë³´ê¸°ë¡œ ì „í™˜", value=True,
                                                    help="ê°€ë¡œí­ì„ ì¤„ì´ê³  ì½ê¸° ì¢‹ê²Œ ì¹´ë“œí˜•ìœ¼ë¡œ í‘œì‹œ")
                            if use_compact:
                                html = render_attachment_cards_html(attach_df, title_col)
                            else:
                                html = render_attachment_table_html(attach_df, title_col, 360, 440, 280)
                            st.markdown(html, unsafe_allow_html=True)

                            xbuf = _BytesIO()
                            with _pd.ExcelWriter(xbuf, engine="openpyxl") as writer:
                                attach_df.to_excel(writer, index=False, sheet_name="attachments")
                            xbuf.seek(0)
                            st.download_button(
                                label="ğŸ“¥ ì²¨ë¶€ ë§í¬ ë§¤íŠ¸ë¦­ìŠ¤ ë‹¤ìš´ë¡œë“œ (Excel)",
                                data=xbuf,
                                file_name=f"{'_'.join(customers)}_ì²¨ë¶€ë§í¬_ë§¤íŠ¸ë¦­ìŠ¤_{_dt.now().strftime('%Y%m%d')}.xlsx",
                                mime="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet"
                            )

                    # ===== GPT ë¶„ì„ =====
                    st.markdown("---")
                    st.subheader("ğŸ¤– GPT ë¶„ì„ (ì—…ë¡œë“œí•œ íŒŒì¼ ê¸°ë°˜)")
                    st.caption("HWP ì—…ë¡œë“œ ì‹œ ìë™ìœ¼ë¡œ **hwp5txt ìš°ì„  â†’ ì‹¤íŒ¨ ì‹œ unoconv(pdf) â†’ PDFë¬¸ì ì¶”ì¶œ** ìˆœìœ¼ë¡œ ì²˜ë¦¬í•©ë‹ˆë‹¤.")
                    src_files = st.file_uploader(
                        "ë¶„ì„í•  íŒŒì¼ ì—…ë¡œë“œ (ì—¬ëŸ¬ ê°œ ê°€ëŠ¥, PDF/HWP/HWPX/DOCX/TXT/CSV/MD/LOG ê¶Œì¥)",
                        type=["pdf","hwp","hwpx","docx","txt","csv","md","log"],
                        accept_multiple_files=True
                    )

                    # ê¸°ì¡´ ë³´ê³ ì„œ ì„¸ì…˜ ë…¸ì¶œ
                    if st.session_state.get("gpt_report_md"):
                        st.markdown("### ğŸ“ GPT ë¶„ì„ ë³´ê³ ì„œ (ì„¸ì…˜ ë³´ì¡´)")
                        st.markdown(st.session_state["gpt_report_md"])

                        base_fname_prev = f"{'_'.join(customers) if customers else 'ì„¸ì…˜'}_GPTë¶„ì„_{_dt.now().strftime('%Y%m%d_%H%M')}"
                        md_bytes_prev = st.session_state["gpt_report_md"].encode("utf-8")
                        col_md_prev, col_pdf_prev = st.columns(2)
                        with col_md_prev:
                            st.download_button(
                                "ğŸ“¥ GPT ë³´ê³ ì„œ ë‹¤ìš´ë¡œë“œ (.md)",
                                data=md_bytes_prev,
                                file_name=f"{base_fname_prev}.md",
                                mime="text/markdown",
                                use_container_width=True,
                            )
                        with col_pdf_prev:
                            pdf_bytes_prev, dbg_prev = markdown_to_pdf_korean(st.session_state["gpt_report_md"], title="GPT ë¶„ì„ ë³´ê³ ì„œ")
                            if pdf_bytes_prev:
                                st.download_button(
                                    "ğŸ“¥ GPT ë³´ê³ ì„œ ë‹¤ìš´ë¡œë“œ (.pdf)",
                                    data=pdf_bytes_prev,
                                    file_name=f"{base_fname_prev}.pdf",
                                    mime="application/pdf",
                                    use_container_width=True,
                                )
                                st.caption(f"PDF ìƒì„± ìƒíƒœ: {dbg_prev}")
                            else:
                                st.error(f"PDF ìƒì„± ì‹¤íŒ¨: {dbg_prev}")

                        files = st.session_state.get("generated_src_pdfs") or []
                        if files:
                            st.markdown("### ğŸ—‚ï¸ ë³€í™˜ëœ ê°„ì´ PDF ë‚´ë ¤ë°›ê¸° (ì„¸ì…˜ ë³´ì¡´)")
                            for i, (fname, pbytes) in enumerate(files):
                                st.download_button(
                                    label=f"ğŸ“¥ {fname}", data=pbytes, file_name=fname,
                                    mime="application/pdf", key=f"dl_srcpdf_prev_{i}", use_container_width=True,
                                )

                    # ë³´ê³ ì„œ ìƒì„±
                    if st.button("ğŸ§  GPT ë¶„ì„ ë³´ê³ ì„œ ìƒì„±", type="primary", use_container_width=True):
                        try:
                            from openai import OpenAI  # ì„¤ì¹˜ ì²´í¬
                        except Exception:
                            st.error("openaiê°€ ì„¤ì¹˜ë˜ì–´ ìˆì§€ ì•ŠìŠµë‹ˆë‹¤. `pip install openai` í›„ ë‹¤ì‹œ ì‹œë„í•˜ì„¸ìš”.")
                        else:
                            if not src_files:
                                st.warning("ë¨¼ì € ë¶„ì„í•  íŒŒì¼ì„ ì—…ë¡œë“œí•˜ì„¸ìš”.")
                            else:
                                with st.spinner("GPTê°€ ì—…ë¡œë“œëœ ìë£Œë¡œ ë³´ê³ ì„œë¥¼ ì‘ì„± ì¤‘..."):
                                    combined_text, logs, generated_pdfs = handle_uploaded_source_files(src_files)
                                    st.write("### ë³€í™˜ ë¡œê·¸")
                                    for line in logs:
                                        st.write("- " + line)
                                    if not combined_text.strip():
                                        st.error("ì—…ë¡œë“œëœ íŒŒì¼ì—ì„œ í…ìŠ¤íŠ¸ë¥¼ ì¶”ì¶œí•˜ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")
                                    else:
                                        safe_extra = _redact_secrets(gpt_extra_req or "")
                                        prompt = (
                                            "ë‹¤ìŒì€ ì¡°ë‹¬/ì…ì°° ê´€ë ¨ ë¬¸ì„œë“¤ì˜ í…ìŠ¤íŠ¸ì…ë‹ˆë‹¤.\n"
                                            "í•µì‹¬ ìš”êµ¬ì‚¬í•­, ê¸°ìˆ /ê°€ê²© í‰ê°€ ë¹„ìœ¨, ê³„ì•½ì¡°ê±´, ì›”ê³¼ ì¼ì„ í¬í•¨í•œ ì •í™•í•œ ì¼ì •(ì…ì°° ë§ˆê°/ê³„ì•½ê¸°ê°„),\n"
                                            "ê³µë™ìˆ˜ê¸‰/í•˜ë„ê¸‰/ê¸´ê¸‰ê³µê³  ì—¬ë¶€, ì£¼ìš” ì¥ë¹„/ìŠ¤í™, ì£¼ìš” êµ¬ê°„, ë°°ì •ì˜ˆì‚°/ì¶”ì •ê°€ê²©/ì˜ˆê°€ ë“±ì„\n"
                                            "í‘œì™€ ë¶ˆë¦¿ìœ¼ë¡œ ìš”ì•½í•˜ì„¸ìš”.\n"
                                            f"ì¶”ê°€ ìš”êµ¬ì‚¬í•­: {safe_extra}\n\n"
                                            "[ë¬¸ì„œ í†µí•© í…ìŠ¤íŠ¸ (ì¼ë¶€ë§Œ ì‚¬ìš©í•´ë„ ë¨)]\n"
                                            f"{combined_text[:180000]}\n"
                                        )
                                        try:
                                            report, used = call_gpt_with_fallback(
                                                [
                                                    {"role": "system", "content": "ë‹¹ì‹ ì€ SKë¸Œë¡œë“œë°´ë“œ ë§ì„¤ê³„/ì¡°ë‹¬ ì œì•ˆ ì»¨ì„¤í„´íŠ¸ì…ë‹ˆë‹¤."},
                                                    {"role": "user", "content": prompt},
                                                ],
                                                model="gpt-4.1",
                                                max_tokens=2000,
                                                temperature=0.4
                                            )
                                            st.markdown("### ğŸ“ GPT ë¶„ì„ ë³´ê³ ì„œ")
                                            st.markdown(report)

                                            st.session_state["gpt_report_md"] = report
                                            st.session_state["generated_src_pdfs"] = generated_pdfs

                                            base_fname = f"{'_'.join(customers)}_GPTë¶„ì„_{_dt.now().strftime('%Y%m%d_%H%M')}"
                                            md_bytes = report.encode("utf-8")

                                            col_md, col_pdf = st.columns(2)
                                            with col_md:
                                                st.download_button(
                                                    "ğŸ“¥ GPT ë³´ê³ ì„œ ë‹¤ìš´ë¡œë“œ (.md)",
                                                    data=md_bytes,
                                                    file_name=f"{base_fname}.md",
                                                    mime="text/markdown",
                                                    use_container_width=True,
                                                )
                                            with col_pdf:
                                                pdf_bytes, dbg = markdown_to_pdf_korean(report, title="GPT ë¶„ì„ ë³´ê³ ì„œ")
                                                if pdf_bytes:
                                                    st.download_button(
                                                        "ğŸ“¥ GPT ë³´ê³ ì„œ ë‹¤ìš´ë¡œë“œ (.pdf)",
                                                        data=pdf_bytes,
                                                        file_name=f"{base_fname}.pdf",
                                                        mime="application/pdf",
                                                        use_container_width=True,
                                                    )
                                                    st.caption(f"PDF ìƒì„± ìƒíƒœ: {dbg}")
                                                else:
                                                    st.error(f"PDF ìƒì„± ì‹¤íŒ¨: {dbg}")

                                            if st.session_state["generated_src_pdfs"]:
                                                st.markdown("---")
                                                st.markdown("### ğŸ—‚ï¸ ë³€í™˜ëœ ê°„ì´ PDF ë‚´ë ¤ë°›ê¸°")
                                                for i, (fname, pbytes) in enumerate(st.session_state["generated_src_pdfs"]):
                                                    st.download_button(
                                                        label=f"ğŸ“¥ {fname}", data=pbytes, file_name=fname,
                                                        mime="application/pdf", key=f"dl_srcpdf_now_{i}",
                                                        use_container_width=True,
                                                    )
                                        except Exception as e:
                                            st.error(f"ë³´ê³ ì„œ ìƒì„± ì¤‘ ì˜¤ë¥˜: {e}")

                    # ===== (2ì°¨) ì°¸ì¡° ì±—ë´‡ =====
                    st.markdown("---")
                    st.subheader("ğŸ’¬ ë³´ê³ ì„œ/í‘œ ì°¸ì¡° ì±—ë´‡")
                    st.caption("ë°©ê¸ˆ ìƒì„±ëœ **ë³´ê³ ì„œ(.md)**ì™€ í˜„ì¬ **í‘œ(ê²€ìƒ‰ ê²°ê³¼ ì¼ë¶€)**ë¥¼ ì»¨í…ìŠ¤íŠ¸ë¡œ ì‚¬ìš©í•©ë‹ˆë‹¤.")
                    question = st.chat_input("ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš” (ì˜ˆ: í•µì‹¬ ë¦¬ìŠ¤í¬ì™€ ì™„í™”ì „ëµë§Œ ì¶”ë ¤ì¤˜)")
                    if question:
                        st.session_state["chat_messages"].append({"role":"user","content":question})
                        ctx_df = result.head(200).copy()
                        with _pd.option_context('display.max_columns', None):
                            df_sample_csv = ctx_df.to_csv(index=False)[:20000]
                        report_ctx = st.session_state.get("gpt_report_md") or "(ì•„ì§ ë³´ê³ ì„œ ì—†ìŒ)"
                        q_prompt = (
                            "ë‹¤ìŒì€ ì»¨í…ìŠ¤íŠ¸ì…ë‹ˆë‹¤.\n"
                            "[ìš”ì•½ ë³´ê³ ì„œ(Markdown)]\n"
                            f"{report_ctx}\n\n"
                            "[í‘œ ë°ì´í„°(ì¼ë¶€ CSV)]\n"
                            f"{df_sample_csv}\n\n"
                            f"ì‚¬ìš©ì ì§ˆë¬¸: {question}\n"
                            "ì»¨í…ìŠ¤íŠ¸ì— ê·¼ê±°í•´ í•œêµ­ì–´ë¡œ ê°„ê²°í•˜ê³  ì¡°ë¦¬ ìˆê²Œ ë‹µí•˜ì„¸ìš”. í‘œ/ë¶ˆë¦¿/ê°„ë‹¨í•œ í…Œì´ë¸” í™œìš© ê°€ëŠ¥.\n"
                        )
                        try:
                            ans, _ = call_gpt_with_fallback(
                                [
                                    {"role":"system","content":"ë‹¹ì‹ ì€ ì¡°ë‹¬/í†µì‹  ì œì•ˆ ë¶„ì„ ì±—ë´‡ì…ë‹ˆë‹¤. ì»¨í…ìŠ¤íŠ¸ë§Œìœ¼ë¡œ ë‹µí•˜ê³  ëª¨ë¥´ë©´ ëª¨ë¥¸ë‹¤ê³  í•˜ì„¸ìš”."},
                                    {"role":"user","content":q_prompt},
                                ],
                                model="gpt-4.1-mini",
                                max_tokens=1200,
                                temperature=0.2
                            )
                            st.session_state["chat_messages"].append({"role":"assistant","content":ans})
                        except Exception as e:
                            st.session_state["chat_messages"].append({"role":"assistant","content":f"ì˜¤ë¥˜: {e}"})

                    for m in st.session_state["chat_messages"]:
                        if m["role"]=="user":
                            st.chat_message("user").markdown(m["content"])
                        else:
                            st.chat_message("assistant").markdown(m["content"])
        else:
            st.info("ê³ ê°ì‚¬ëª…ì„ ì…ë ¥í•˜ë©´ ìë™ í•„í„°ë§ë©ë‹ˆë‹¤.")


if __name__ == "__main__":
    # ì „ì—­ ì„¤ì • & ê³µìš© ìœ í‹¸ (app_part1.py)
    st.set_page_config(
        page_title="ì¡°ë‹¬ì…ì°° ë¶„ì„ ì‹œìŠ¤í…œ",
        layout="wide",
        initial_sidebar_state="expanded",
    )
    st.markdown(
        """
        <meta name="robots" content="noindex,nofollow">
        <meta name="googlebot" content="noindex,nofollow">
        """,
        unsafe_allow_html=True,
    )

    main()
